# A3C Agent for Kung Fu Game

This project demonstrates the application of the Asynchronous Advantage Actor-Critic (A3C) algorithm to train an AI agent to play the classic arcade game Kung Fu. The goal is to develop an agent that can effectively navigate the game environment, defeat enemies, and advance through levels.

## Project Overview

- **Environment:** The Kung Fu environment, a classic arcade game.
- **Algorithm:** Asynchronous Advantage Actor-Critic (A3C), a reinforcement learning algorithm that uses parallel actor-learners to stabilize and accelerate training.
- **Objective:** Train an agent to master Kung Fu by optimizing its actions through asynchronous learning and exploration.

## Features

- Implementation of the A3C algorithm with parallel actor-learners.
- Integration of environment interaction and reward accumulation for training the agent.
- Visualization of training progress and agent's performance in real-time.
- Evaluation of the trained agent's ability to play Kung Fu in different levels and scenarios.

## How to Run

1. **Access the Colab Notebook:** Open the A3C for Kung Fu Colab Notebook to view and run the project code directly in Google Colab.
   
2. **Run the Cells:** Execute the cells in the notebook sequentially to start training the agent and monitor its performance.
   
3. **Evaluate the Agent:** After training, use the evaluation cells to assess the agent's performance in playing Kung Fu.

## Results

To see the results of this project, visit the [Google Colab notebook](https://colab.research.google.com/github/dhritishetty/A3C-for-Kung-Fu/blob/main/A3C%20for%20Kung%20Fu.ipynb) where you can interactively view the training process and visualizations.
